{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EonKim\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from https://www.openml.org/d/554\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (70000, 784)\n",
      "y shape: (70000,)\n",
      "Single sample shape: (28, 28)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f'X shape: {X.shape}')\n",
    "print(f'y shape: {y.shape}')\n",
    "\n",
    "single_sample = X.loc[0,:].values.reshape(28,28)\n",
    "print(f'Single sample shape: {single_sample.shape}')\n",
    "\n",
    "plt.gray()\n",
    "plt.matshow(single_sample)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shape: (60000, 784)\n",
      "Testing shape (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "train_samples = 60000\n",
    "random_state = check_random_state(0)\n",
    "permutation = random_state.permutation(X.shape[0])\n",
    "\n",
    "X = X.iloc[permutation]\n",
    "y = y.iloc[permutation]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, train_size=train_samples, test_size=10000, random_state=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "\n",
    "print(f'Training shape: {X_train.shape}')\n",
    "print(f'Testing shape {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Layer \n",
    "\n",
    "$$ O = \\frac{W - K + 2 \\cdot P}{S} + 1 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Input shape: \t [28, 28]\n",
      "==================================================\n",
      "Padding shape: \t 0\n",
      "Output shape: \t 24\n",
      "Region shape: \t [5, 5]\n",
      "Kernel shape: \t [8, 5, 5]\n",
      "Single Slide: \t [8]\n",
      "==================================================\n",
      "Conv (f) shape: \t [24, 24, 8]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "class ConvolutionalLayer:\n",
    "    def __init__(self, num_kernels, kernel_shape):\n",
    "        self.num_kernels = num_kernels \n",
    "        self.kernel_shape = kernel_shape \n",
    "        self.k = self.kernel_shape[0]\n",
    "        \n",
    "        self.kernel_theta = torch.randn(self.num_kernels, self.kernel_shape[0], self.kernel_shape[1])\n",
    "    \n",
    "    def slider(self, image):\n",
    "        h, w = image.shape\n",
    "        for h_idx in range(h - (self.k - 1)): # h: height, w: width\n",
    "            for w_idx in range(w - (self.k - 1)):\n",
    "                single_slide_area = image[h_idx:(h_idx + self.k), w_idx:(w_idx + self.k)]\n",
    "                yield single_slide_area, h_idx, w_idx \n",
    "    \n",
    "    def forward(self, images):\n",
    "        \n",
    "        assert single_sample.dim() == 2\n",
    "        \n",
    "        _, w = images.shape \n",
    "        p = 0\n",
    "        o = (w - self.k) + 1\n",
    "        print('Padding shape: \\t', p)\n",
    "        print('Output shape: \\t', o)\n",
    "        \n",
    "        output = torch.zeros((o, o, self.num_kernels))\n",
    "        \n",
    "        for single_slide_area, h_idx, w_idx in self.slider(images):\n",
    "            if h_idx == 0 and w_idx == 0 :\n",
    "                print('Region shape: \\t', list(single_slide_area.shape))\n",
    "                print('Kernel shape: \\t', list(self.kernel_theta.shape))\n",
    "                print('Single Slide: \\t', list(output[h_idx, w_idx].shape))\n",
    "            \n",
    "            output[h_idx, w_idx] = torch.sum(single_slide_area * self.kernel_theta, axis=(1, 2))\n",
    "        output = 1. / (1. + torch.exp(-output))\n",
    "    \n",
    "        return output\n",
    "\n",
    "single_sample = X.loc[0].values.reshape(28, 28)\n",
    "single_sample = torch.tensor(single_sample)\n",
    "\n",
    "print('='*50)\n",
    "print(f'Input shape: \\t {list(single_sample.shape)}')\n",
    "print('='*50)\n",
    "\n",
    "Conv = ConvolutionalLayer(num_kernels=8, kernel_shape=(5, 5))\n",
    "output = Conv.forward(single_sample)\n",
    "\n",
    "print('='*50)\n",
    "print(f'Conv (f) shape: \\t {list(output.shape)}')\n",
    "print('='*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Input shape: \t [24, 24, 8]\n",
      "==================================================\n",
      "Lecun initialization SD: 0.1\n",
      "input shape: \t torch.Size([1, 4608])\n",
      "weight shape: \t torch.Size([4608, 10])\n",
      "bias shape \t torch.Size([10])\n",
      "==================================================\n",
      "Affine & Soft(arg)max (f) shape: \t [1, 10]\n",
      "==================================================\n",
      "Probas: 0.112889 0.000923 0.000026 0.067042 0.000001 1.890935e-07 0.814461 0.001939 0.000121 0.002597\n"
     ]
    }
   ],
   "source": [
    "class AffineSoftmaxLayer:\n",
    "    def __init__(self, affine_weight_shape):\n",
    "        self.affine_weight_shape = affine_weight_shape\n",
    "        self.w = torch.zeros(self.affine_weight_shape[0] * self.affine_weight_shape[1] * self.affine_weight_shape[2], self.affine_weight_shape[3])\n",
    "        self.b = torch.zeros(self.affine_weight_shape[3])\n",
    "        \n",
    "        \n",
    "        print(f'Lecun initialization SD: {1/self.affine_weight_shape[3]}')\n",
    "        self.w = torch.nn.init.normal_(self.w, mean=0, std=1/self.affine_weight_shape[3])\n",
    "        self.b = torch.nn.init.normal_(self.b, mean=0, std=1/self.affine_weight_shape[3])\n",
    "        \n",
    "    def forward(self, image):\n",
    "        image = image.reshape(1, -1)\n",
    "        print(f'input shape: \\t {image.shape}')\n",
    "        print(f'weight shape: \\t {self.w.shape}')\n",
    "        print(f'bias shape \\t {self.b.shape}')\n",
    "        logits = torch.mm(image, self.w) + self.b \n",
    "        proba = torch.exp(logits) / torch.sum(torch.exp(logits))\n",
    "        return proba \n",
    "\n",
    "\n",
    "print('='*50)\n",
    "print(f'Input shape: \\t {list(output.shape)}')\n",
    "print('='*50)\n",
    "\n",
    "# Forward: Affine and Softmax\n",
    "affinesoftmax = AffineSoftmaxLayer(affine_weight_shape=(output.shape[0], output.shape[1], output.shape[2], len(np.unique(Y_train))))\n",
    "output = affinesoftmax.forward(output)\n",
    "\n",
    "print('='*50)\n",
    "print(f'Affine & Soft(arg)max (f) shape: \\t {list(output.shape)}')\n",
    "print('='*50)\n",
    "\n",
    "print(f'Probas: {pd.DataFrame(output.numpy()).to_string(index=False, header=False)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "116e0bc72a1820dee7c1d3f3e708778f7416cc41eb6b2ea33b8e8b62fc39e31e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
