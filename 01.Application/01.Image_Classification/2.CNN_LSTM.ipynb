{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[COCO](https://cocodataset.org/) 여기서 데이터를 다운받을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pycocotools==2.0.0\n",
    "# !conda install -c conda-forge pycocotools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sys, os, pickle \n",
    "import numpy as np \n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "from pycocotools.coco import COCO \n",
    "import torchvision.models as model \n",
    "import torchvision.transforms as transform \n",
    "from torch.nn.utils.rnn import pack_padded_sequence \n",
    "# from settings import * \n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "sys.path.append('/workspace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab(object):\n",
    "    \"\"\"Simple vocabulary wrapper.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.w2i = {}\n",
    "        self.i2w = {}\n",
    "        self.index = 0\n",
    " \n",
    "    def __call__(self, token):\n",
    "        if not token in self.w2i:\n",
    "            return self.w2i['']\n",
    "        return self.w2i[token]\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.w2i)\n",
    "    def add_token(self, token):\n",
    "        if not token in self.w2i:\n",
    "            self.w2i[token] = self.index\n",
    "            self.i2w[self.index] = token\n",
    "            self.index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocaburary(json, threshold):\n",
    "    coco = COCO(json)\n",
    "    counter = nltk.Counter()\n",
    "    ids = coco.anns.keys() \n",
    "    for i, id in enumerate(ids):\n",
    "        caption = str(coco.anns[id]['caption'])\n",
    "        tokens = nltk.tokenize.word_tokenize(caption.lower())\n",
    "        \n",
    "        counter.update(tokens)\n",
    "        \n",
    "        if (i+1) % 1000 == 0:\n",
    "            print(f'[{i+1}/{len(ids)}] Tokenized the captions.')\n",
    "        \n",
    "    tokens = [token for token, cnt in counter.items() if cnt >= threshold]\n",
    "    vocab = Vocab()\n",
    "    vocab.add_token('<pad>')\n",
    "    vocab.add_token('<start>')\n",
    "    vocab.add_token('<end>')\n",
    "    vocab.add_token('<unk>')\n",
    "    \n",
    "    for token in tokens:\n",
    "        vocab.add_token(token)\n",
    "        \n",
    "    return vocab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataType = 'val2017'\n",
    "annFile = f'../annotations/instances_{dataType}.json'\n",
    "# annFile = f'../annotations/instances_{dataType}.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.27s)\n",
      "creating index...\n",
      "index created!\n",
      "[1000/591753] Tokenized the captions.\n",
      "[2000/591753] Tokenized the captions.\n",
      "[3000/591753] Tokenized the captions.\n",
      "[4000/591753] Tokenized the captions.\n",
      "[5000/591753] Tokenized the captions.\n",
      "[6000/591753] Tokenized the captions.\n",
      "[7000/591753] Tokenized the captions.\n",
      "[8000/591753] Tokenized the captions.\n",
      "[9000/591753] Tokenized the captions.\n",
      "[10000/591753] Tokenized the captions.\n",
      "[11000/591753] Tokenized the captions.\n",
      "[12000/591753] Tokenized the captions.\n",
      "[13000/591753] Tokenized the captions.\n",
      "[14000/591753] Tokenized the captions.\n",
      "[15000/591753] Tokenized the captions.\n",
      "[16000/591753] Tokenized the captions.\n",
      "[17000/591753] Tokenized the captions.\n",
      "[18000/591753] Tokenized the captions.\n",
      "[19000/591753] Tokenized the captions.\n",
      "[20000/591753] Tokenized the captions.\n",
      "[21000/591753] Tokenized the captions.\n",
      "[22000/591753] Tokenized the captions.\n",
      "[23000/591753] Tokenized the captions.\n",
      "[24000/591753] Tokenized the captions.\n",
      "[25000/591753] Tokenized the captions.\n",
      "[26000/591753] Tokenized the captions.\n",
      "[27000/591753] Tokenized the captions.\n",
      "[28000/591753] Tokenized the captions.\n",
      "[29000/591753] Tokenized the captions.\n",
      "[30000/591753] Tokenized the captions.\n",
      "[31000/591753] Tokenized the captions.\n",
      "[32000/591753] Tokenized the captions.\n",
      "[33000/591753] Tokenized the captions.\n",
      "[34000/591753] Tokenized the captions.\n",
      "[35000/591753] Tokenized the captions.\n",
      "[36000/591753] Tokenized the captions.\n",
      "[37000/591753] Tokenized the captions.\n",
      "[38000/591753] Tokenized the captions.\n",
      "[39000/591753] Tokenized the captions.\n",
      "[40000/591753] Tokenized the captions.\n",
      "[41000/591753] Tokenized the captions.\n",
      "[42000/591753] Tokenized the captions.\n",
      "[43000/591753] Tokenized the captions.\n",
      "[44000/591753] Tokenized the captions.\n",
      "[45000/591753] Tokenized the captions.\n",
      "[46000/591753] Tokenized the captions.\n",
      "[47000/591753] Tokenized the captions.\n",
      "[48000/591753] Tokenized the captions.\n",
      "[49000/591753] Tokenized the captions.\n",
      "[50000/591753] Tokenized the captions.\n",
      "[51000/591753] Tokenized the captions.\n",
      "[52000/591753] Tokenized the captions.\n",
      "[53000/591753] Tokenized the captions.\n",
      "[54000/591753] Tokenized the captions.\n",
      "[55000/591753] Tokenized the captions.\n",
      "[56000/591753] Tokenized the captions.\n",
      "[57000/591753] Tokenized the captions.\n",
      "[58000/591753] Tokenized the captions.\n",
      "[59000/591753] Tokenized the captions.\n",
      "[60000/591753] Tokenized the captions.\n",
      "[61000/591753] Tokenized the captions.\n",
      "[62000/591753] Tokenized the captions.\n",
      "[63000/591753] Tokenized the captions.\n",
      "[64000/591753] Tokenized the captions.\n",
      "[65000/591753] Tokenized the captions.\n",
      "[66000/591753] Tokenized the captions.\n",
      "[67000/591753] Tokenized the captions.\n",
      "[68000/591753] Tokenized the captions.\n",
      "[69000/591753] Tokenized the captions.\n",
      "[70000/591753] Tokenized the captions.\n",
      "[71000/591753] Tokenized the captions.\n",
      "[72000/591753] Tokenized the captions.\n",
      "[73000/591753] Tokenized the captions.\n",
      "[74000/591753] Tokenized the captions.\n",
      "[75000/591753] Tokenized the captions.\n",
      "[76000/591753] Tokenized the captions.\n",
      "[77000/591753] Tokenized the captions.\n",
      "[78000/591753] Tokenized the captions.\n",
      "[79000/591753] Tokenized the captions.\n",
      "[80000/591753] Tokenized the captions.\n",
      "[81000/591753] Tokenized the captions.\n",
      "[82000/591753] Tokenized the captions.\n",
      "[83000/591753] Tokenized the captions.\n",
      "[84000/591753] Tokenized the captions.\n",
      "[85000/591753] Tokenized the captions.\n",
      "[86000/591753] Tokenized the captions.\n",
      "[87000/591753] Tokenized the captions.\n",
      "[88000/591753] Tokenized the captions.\n",
      "[89000/591753] Tokenized the captions.\n",
      "[90000/591753] Tokenized the captions.\n",
      "[91000/591753] Tokenized the captions.\n",
      "[92000/591753] Tokenized the captions.\n",
      "[93000/591753] Tokenized the captions.\n",
      "[94000/591753] Tokenized the captions.\n",
      "[95000/591753] Tokenized the captions.\n",
      "[96000/591753] Tokenized the captions.\n",
      "[97000/591753] Tokenized the captions.\n",
      "[98000/591753] Tokenized the captions.\n",
      "[99000/591753] Tokenized the captions.\n",
      "[100000/591753] Tokenized the captions.\n",
      "[101000/591753] Tokenized the captions.\n",
      "[102000/591753] Tokenized the captions.\n",
      "[103000/591753] Tokenized the captions.\n",
      "[104000/591753] Tokenized the captions.\n",
      "[105000/591753] Tokenized the captions.\n",
      "[106000/591753] Tokenized the captions.\n",
      "[107000/591753] Tokenized the captions.\n",
      "[108000/591753] Tokenized the captions.\n",
      "[109000/591753] Tokenized the captions.\n",
      "[110000/591753] Tokenized the captions.\n",
      "[111000/591753] Tokenized the captions.\n",
      "[112000/591753] Tokenized the captions.\n",
      "[113000/591753] Tokenized the captions.\n",
      "[114000/591753] Tokenized the captions.\n",
      "[115000/591753] Tokenized the captions.\n",
      "[116000/591753] Tokenized the captions.\n",
      "[117000/591753] Tokenized the captions.\n",
      "[118000/591753] Tokenized the captions.\n",
      "[119000/591753] Tokenized the captions.\n",
      "[120000/591753] Tokenized the captions.\n",
      "[121000/591753] Tokenized the captions.\n",
      "[122000/591753] Tokenized the captions.\n",
      "[123000/591753] Tokenized the captions.\n",
      "[124000/591753] Tokenized the captions.\n",
      "[125000/591753] Tokenized the captions.\n",
      "[126000/591753] Tokenized the captions.\n",
      "[127000/591753] Tokenized the captions.\n",
      "[128000/591753] Tokenized the captions.\n",
      "[129000/591753] Tokenized the captions.\n",
      "[130000/591753] Tokenized the captions.\n",
      "[131000/591753] Tokenized the captions.\n",
      "[132000/591753] Tokenized the captions.\n",
      "[133000/591753] Tokenized the captions.\n",
      "[134000/591753] Tokenized the captions.\n",
      "[135000/591753] Tokenized the captions.\n",
      "[136000/591753] Tokenized the captions.\n",
      "[137000/591753] Tokenized the captions.\n",
      "[138000/591753] Tokenized the captions.\n",
      "[139000/591753] Tokenized the captions.\n",
      "[140000/591753] Tokenized the captions.\n",
      "[141000/591753] Tokenized the captions.\n",
      "[142000/591753] Tokenized the captions.\n",
      "[143000/591753] Tokenized the captions.\n",
      "[144000/591753] Tokenized the captions.\n",
      "[145000/591753] Tokenized the captions.\n",
      "[146000/591753] Tokenized the captions.\n",
      "[147000/591753] Tokenized the captions.\n",
      "[148000/591753] Tokenized the captions.\n",
      "[149000/591753] Tokenized the captions.\n",
      "[150000/591753] Tokenized the captions.\n",
      "[151000/591753] Tokenized the captions.\n",
      "[152000/591753] Tokenized the captions.\n",
      "[153000/591753] Tokenized the captions.\n",
      "[154000/591753] Tokenized the captions.\n",
      "[155000/591753] Tokenized the captions.\n",
      "[156000/591753] Tokenized the captions.\n",
      "[157000/591753] Tokenized the captions.\n",
      "[158000/591753] Tokenized the captions.\n",
      "[159000/591753] Tokenized the captions.\n",
      "[160000/591753] Tokenized the captions.\n",
      "[161000/591753] Tokenized the captions.\n",
      "[162000/591753] Tokenized the captions.\n",
      "[163000/591753] Tokenized the captions.\n",
      "[164000/591753] Tokenized the captions.\n",
      "[165000/591753] Tokenized the captions.\n",
      "[166000/591753] Tokenized the captions.\n",
      "[167000/591753] Tokenized the captions.\n",
      "[168000/591753] Tokenized the captions.\n",
      "[169000/591753] Tokenized the captions.\n",
      "[170000/591753] Tokenized the captions.\n",
      "[171000/591753] Tokenized the captions.\n",
      "[172000/591753] Tokenized the captions.\n",
      "[173000/591753] Tokenized the captions.\n",
      "[174000/591753] Tokenized the captions.\n",
      "[175000/591753] Tokenized the captions.\n",
      "[176000/591753] Tokenized the captions.\n",
      "[177000/591753] Tokenized the captions.\n",
      "[178000/591753] Tokenized the captions.\n",
      "[179000/591753] Tokenized the captions.\n",
      "[180000/591753] Tokenized the captions.\n",
      "[181000/591753] Tokenized the captions.\n",
      "[182000/591753] Tokenized the captions.\n",
      "[183000/591753] Tokenized the captions.\n",
      "[184000/591753] Tokenized the captions.\n",
      "[185000/591753] Tokenized the captions.\n",
      "[186000/591753] Tokenized the captions.\n",
      "[187000/591753] Tokenized the captions.\n",
      "[188000/591753] Tokenized the captions.\n",
      "[189000/591753] Tokenized the captions.\n",
      "[190000/591753] Tokenized the captions.\n",
      "[191000/591753] Tokenized the captions.\n",
      "[192000/591753] Tokenized the captions.\n",
      "[193000/591753] Tokenized the captions.\n",
      "[194000/591753] Tokenized the captions.\n",
      "[195000/591753] Tokenized the captions.\n",
      "[196000/591753] Tokenized the captions.\n",
      "[197000/591753] Tokenized the captions.\n",
      "[198000/591753] Tokenized the captions.\n",
      "[199000/591753] Tokenized the captions.\n",
      "[200000/591753] Tokenized the captions.\n",
      "[201000/591753] Tokenized the captions.\n",
      "[202000/591753] Tokenized the captions.\n",
      "[203000/591753] Tokenized the captions.\n",
      "[204000/591753] Tokenized the captions.\n",
      "[205000/591753] Tokenized the captions.\n",
      "[206000/591753] Tokenized the captions.\n",
      "[207000/591753] Tokenized the captions.\n",
      "[208000/591753] Tokenized the captions.\n",
      "[209000/591753] Tokenized the captions.\n",
      "[210000/591753] Tokenized the captions.\n",
      "[211000/591753] Tokenized the captions.\n",
      "[212000/591753] Tokenized the captions.\n",
      "[213000/591753] Tokenized the captions.\n",
      "[214000/591753] Tokenized the captions.\n",
      "[215000/591753] Tokenized the captions.\n",
      "[216000/591753] Tokenized the captions.\n",
      "[217000/591753] Tokenized the captions.\n",
      "[218000/591753] Tokenized the captions.\n",
      "[219000/591753] Tokenized the captions.\n",
      "[220000/591753] Tokenized the captions.\n",
      "[221000/591753] Tokenized the captions.\n",
      "[222000/591753] Tokenized the captions.\n",
      "[223000/591753] Tokenized the captions.\n",
      "[224000/591753] Tokenized the captions.\n",
      "[225000/591753] Tokenized the captions.\n",
      "[226000/591753] Tokenized the captions.\n",
      "[227000/591753] Tokenized the captions.\n",
      "[228000/591753] Tokenized the captions.\n",
      "[229000/591753] Tokenized the captions.\n",
      "[230000/591753] Tokenized the captions.\n",
      "[231000/591753] Tokenized the captions.\n",
      "[232000/591753] Tokenized the captions.\n",
      "[233000/591753] Tokenized the captions.\n",
      "[234000/591753] Tokenized the captions.\n",
      "[235000/591753] Tokenized the captions.\n",
      "[236000/591753] Tokenized the captions.\n",
      "[237000/591753] Tokenized the captions.\n",
      "[238000/591753] Tokenized the captions.\n",
      "[239000/591753] Tokenized the captions.\n",
      "[240000/591753] Tokenized the captions.\n",
      "[241000/591753] Tokenized the captions.\n",
      "[242000/591753] Tokenized the captions.\n",
      "[243000/591753] Tokenized the captions.\n",
      "[244000/591753] Tokenized the captions.\n",
      "[245000/591753] Tokenized the captions.\n",
      "[246000/591753] Tokenized the captions.\n",
      "[247000/591753] Tokenized the captions.\n",
      "[248000/591753] Tokenized the captions.\n",
      "[249000/591753] Tokenized the captions.\n",
      "[250000/591753] Tokenized the captions.\n",
      "[251000/591753] Tokenized the captions.\n",
      "[252000/591753] Tokenized the captions.\n",
      "[253000/591753] Tokenized the captions.\n",
      "[254000/591753] Tokenized the captions.\n",
      "[255000/591753] Tokenized the captions.\n",
      "[256000/591753] Tokenized the captions.\n",
      "[257000/591753] Tokenized the captions.\n",
      "[258000/591753] Tokenized the captions.\n",
      "[259000/591753] Tokenized the captions.\n",
      "[260000/591753] Tokenized the captions.\n",
      "[261000/591753] Tokenized the captions.\n",
      "[262000/591753] Tokenized the captions.\n",
      "[263000/591753] Tokenized the captions.\n",
      "[264000/591753] Tokenized the captions.\n",
      "[265000/591753] Tokenized the captions.\n",
      "[266000/591753] Tokenized the captions.\n",
      "[267000/591753] Tokenized the captions.\n",
      "[268000/591753] Tokenized the captions.\n",
      "[269000/591753] Tokenized the captions.\n",
      "[270000/591753] Tokenized the captions.\n",
      "[271000/591753] Tokenized the captions.\n",
      "[272000/591753] Tokenized the captions.\n",
      "[273000/591753] Tokenized the captions.\n",
      "[274000/591753] Tokenized the captions.\n",
      "[275000/591753] Tokenized the captions.\n",
      "[276000/591753] Tokenized the captions.\n",
      "[277000/591753] Tokenized the captions.\n",
      "[278000/591753] Tokenized the captions.\n",
      "[279000/591753] Tokenized the captions.\n",
      "[280000/591753] Tokenized the captions.\n",
      "[281000/591753] Tokenized the captions.\n",
      "[282000/591753] Tokenized the captions.\n",
      "[283000/591753] Tokenized the captions.\n",
      "[284000/591753] Tokenized the captions.\n",
      "[285000/591753] Tokenized the captions.\n",
      "[286000/591753] Tokenized the captions.\n",
      "[287000/591753] Tokenized the captions.\n",
      "[288000/591753] Tokenized the captions.\n",
      "[289000/591753] Tokenized the captions.\n",
      "[290000/591753] Tokenized the captions.\n",
      "[291000/591753] Tokenized the captions.\n",
      "[292000/591753] Tokenized the captions.\n",
      "[293000/591753] Tokenized the captions.\n",
      "[294000/591753] Tokenized the captions.\n",
      "[295000/591753] Tokenized the captions.\n",
      "[296000/591753] Tokenized the captions.\n",
      "[297000/591753] Tokenized the captions.\n",
      "[298000/591753] Tokenized the captions.\n",
      "[299000/591753] Tokenized the captions.\n",
      "[300000/591753] Tokenized the captions.\n",
      "[301000/591753] Tokenized the captions.\n",
      "[302000/591753] Tokenized the captions.\n",
      "[303000/591753] Tokenized the captions.\n",
      "[304000/591753] Tokenized the captions.\n",
      "[305000/591753] Tokenized the captions.\n",
      "[306000/591753] Tokenized the captions.\n",
      "[307000/591753] Tokenized the captions.\n",
      "[308000/591753] Tokenized the captions.\n",
      "[309000/591753] Tokenized the captions.\n",
      "[310000/591753] Tokenized the captions.\n",
      "[311000/591753] Tokenized the captions.\n",
      "[312000/591753] Tokenized the captions.\n",
      "[313000/591753] Tokenized the captions.\n",
      "[314000/591753] Tokenized the captions.\n",
      "[315000/591753] Tokenized the captions.\n",
      "[316000/591753] Tokenized the captions.\n",
      "[317000/591753] Tokenized the captions.\n",
      "[318000/591753] Tokenized the captions.\n",
      "[319000/591753] Tokenized the captions.\n",
      "[320000/591753] Tokenized the captions.\n",
      "[321000/591753] Tokenized the captions.\n",
      "[322000/591753] Tokenized the captions.\n",
      "[323000/591753] Tokenized the captions.\n",
      "[324000/591753] Tokenized the captions.\n",
      "[325000/591753] Tokenized the captions.\n",
      "[326000/591753] Tokenized the captions.\n",
      "[327000/591753] Tokenized the captions.\n",
      "[328000/591753] Tokenized the captions.\n",
      "[329000/591753] Tokenized the captions.\n",
      "[330000/591753] Tokenized the captions.\n",
      "[331000/591753] Tokenized the captions.\n",
      "[332000/591753] Tokenized the captions.\n",
      "[333000/591753] Tokenized the captions.\n",
      "[334000/591753] Tokenized the captions.\n",
      "[335000/591753] Tokenized the captions.\n",
      "[336000/591753] Tokenized the captions.\n",
      "[337000/591753] Tokenized the captions.\n",
      "[338000/591753] Tokenized the captions.\n",
      "[339000/591753] Tokenized the captions.\n",
      "[340000/591753] Tokenized the captions.\n",
      "[341000/591753] Tokenized the captions.\n",
      "[342000/591753] Tokenized the captions.\n",
      "[343000/591753] Tokenized the captions.\n",
      "[344000/591753] Tokenized the captions.\n",
      "[345000/591753] Tokenized the captions.\n",
      "[346000/591753] Tokenized the captions.\n",
      "[347000/591753] Tokenized the captions.\n",
      "[348000/591753] Tokenized the captions.\n",
      "[349000/591753] Tokenized the captions.\n",
      "[350000/591753] Tokenized the captions.\n",
      "[351000/591753] Tokenized the captions.\n",
      "[352000/591753] Tokenized the captions.\n",
      "[353000/591753] Tokenized the captions.\n",
      "[354000/591753] Tokenized the captions.\n",
      "[355000/591753] Tokenized the captions.\n",
      "[356000/591753] Tokenized the captions.\n",
      "[357000/591753] Tokenized the captions.\n",
      "[358000/591753] Tokenized the captions.\n",
      "[359000/591753] Tokenized the captions.\n",
      "[360000/591753] Tokenized the captions.\n",
      "[361000/591753] Tokenized the captions.\n",
      "[362000/591753] Tokenized the captions.\n",
      "[363000/591753] Tokenized the captions.\n",
      "[364000/591753] Tokenized the captions.\n",
      "[365000/591753] Tokenized the captions.\n",
      "[366000/591753] Tokenized the captions.\n",
      "[367000/591753] Tokenized the captions.\n",
      "[368000/591753] Tokenized the captions.\n",
      "[369000/591753] Tokenized the captions.\n",
      "[370000/591753] Tokenized the captions.\n",
      "[371000/591753] Tokenized the captions.\n",
      "[372000/591753] Tokenized the captions.\n",
      "[373000/591753] Tokenized the captions.\n",
      "[374000/591753] Tokenized the captions.\n",
      "[375000/591753] Tokenized the captions.\n",
      "[376000/591753] Tokenized the captions.\n",
      "[377000/591753] Tokenized the captions.\n",
      "[378000/591753] Tokenized the captions.\n",
      "[379000/591753] Tokenized the captions.\n",
      "[380000/591753] Tokenized the captions.\n",
      "[381000/591753] Tokenized the captions.\n",
      "[382000/591753] Tokenized the captions.\n",
      "[383000/591753] Tokenized the captions.\n",
      "[384000/591753] Tokenized the captions.\n",
      "[385000/591753] Tokenized the captions.\n",
      "[386000/591753] Tokenized the captions.\n",
      "[387000/591753] Tokenized the captions.\n",
      "[388000/591753] Tokenized the captions.\n",
      "[389000/591753] Tokenized the captions.\n",
      "[390000/591753] Tokenized the captions.\n",
      "[391000/591753] Tokenized the captions.\n",
      "[392000/591753] Tokenized the captions.\n",
      "[393000/591753] Tokenized the captions.\n",
      "[394000/591753] Tokenized the captions.\n",
      "[395000/591753] Tokenized the captions.\n",
      "[396000/591753] Tokenized the captions.\n",
      "[397000/591753] Tokenized the captions.\n",
      "[398000/591753] Tokenized the captions.\n",
      "[399000/591753] Tokenized the captions.\n",
      "[400000/591753] Tokenized the captions.\n",
      "[401000/591753] Tokenized the captions.\n",
      "[402000/591753] Tokenized the captions.\n",
      "[403000/591753] Tokenized the captions.\n",
      "[404000/591753] Tokenized the captions.\n",
      "[405000/591753] Tokenized the captions.\n",
      "[406000/591753] Tokenized the captions.\n",
      "[407000/591753] Tokenized the captions.\n",
      "[408000/591753] Tokenized the captions.\n",
      "[409000/591753] Tokenized the captions.\n",
      "[410000/591753] Tokenized the captions.\n",
      "[411000/591753] Tokenized the captions.\n",
      "[412000/591753] Tokenized the captions.\n",
      "[413000/591753] Tokenized the captions.\n",
      "[414000/591753] Tokenized the captions.\n",
      "[415000/591753] Tokenized the captions.\n",
      "[416000/591753] Tokenized the captions.\n",
      "[417000/591753] Tokenized the captions.\n",
      "[418000/591753] Tokenized the captions.\n",
      "[419000/591753] Tokenized the captions.\n",
      "[420000/591753] Tokenized the captions.\n",
      "[421000/591753] Tokenized the captions.\n",
      "[422000/591753] Tokenized the captions.\n",
      "[423000/591753] Tokenized the captions.\n",
      "[424000/591753] Tokenized the captions.\n",
      "[425000/591753] Tokenized the captions.\n",
      "[426000/591753] Tokenized the captions.\n",
      "[427000/591753] Tokenized the captions.\n",
      "[428000/591753] Tokenized the captions.\n",
      "[429000/591753] Tokenized the captions.\n",
      "[430000/591753] Tokenized the captions.\n",
      "[431000/591753] Tokenized the captions.\n",
      "[432000/591753] Tokenized the captions.\n",
      "[433000/591753] Tokenized the captions.\n",
      "[434000/591753] Tokenized the captions.\n",
      "[435000/591753] Tokenized the captions.\n",
      "[436000/591753] Tokenized the captions.\n",
      "[437000/591753] Tokenized the captions.\n",
      "[438000/591753] Tokenized the captions.\n",
      "[439000/591753] Tokenized the captions.\n",
      "[440000/591753] Tokenized the captions.\n",
      "[441000/591753] Tokenized the captions.\n",
      "[442000/591753] Tokenized the captions.\n",
      "[443000/591753] Tokenized the captions.\n",
      "[444000/591753] Tokenized the captions.\n",
      "[445000/591753] Tokenized the captions.\n",
      "[446000/591753] Tokenized the captions.\n",
      "[447000/591753] Tokenized the captions.\n",
      "[448000/591753] Tokenized the captions.\n",
      "[449000/591753] Tokenized the captions.\n",
      "[450000/591753] Tokenized the captions.\n",
      "[451000/591753] Tokenized the captions.\n",
      "[452000/591753] Tokenized the captions.\n",
      "[453000/591753] Tokenized the captions.\n",
      "[454000/591753] Tokenized the captions.\n",
      "[455000/591753] Tokenized the captions.\n",
      "[456000/591753] Tokenized the captions.\n",
      "[457000/591753] Tokenized the captions.\n",
      "[458000/591753] Tokenized the captions.\n",
      "[459000/591753] Tokenized the captions.\n",
      "[460000/591753] Tokenized the captions.\n",
      "[461000/591753] Tokenized the captions.\n",
      "[462000/591753] Tokenized the captions.\n",
      "[463000/591753] Tokenized the captions.\n",
      "[464000/591753] Tokenized the captions.\n",
      "[465000/591753] Tokenized the captions.\n",
      "[466000/591753] Tokenized the captions.\n",
      "[467000/591753] Tokenized the captions.\n",
      "[468000/591753] Tokenized the captions.\n",
      "[469000/591753] Tokenized the captions.\n",
      "[470000/591753] Tokenized the captions.\n",
      "[471000/591753] Tokenized the captions.\n",
      "[472000/591753] Tokenized the captions.\n",
      "[473000/591753] Tokenized the captions.\n",
      "[474000/591753] Tokenized the captions.\n",
      "[475000/591753] Tokenized the captions.\n",
      "[476000/591753] Tokenized the captions.\n",
      "[477000/591753] Tokenized the captions.\n",
      "[478000/591753] Tokenized the captions.\n",
      "[479000/591753] Tokenized the captions.\n",
      "[480000/591753] Tokenized the captions.\n",
      "[481000/591753] Tokenized the captions.\n",
      "[482000/591753] Tokenized the captions.\n",
      "[483000/591753] Tokenized the captions.\n",
      "[484000/591753] Tokenized the captions.\n",
      "[485000/591753] Tokenized the captions.\n",
      "[486000/591753] Tokenized the captions.\n",
      "[487000/591753] Tokenized the captions.\n",
      "[488000/591753] Tokenized the captions.\n",
      "[489000/591753] Tokenized the captions.\n",
      "[490000/591753] Tokenized the captions.\n",
      "[491000/591753] Tokenized the captions.\n",
      "[492000/591753] Tokenized the captions.\n",
      "[493000/591753] Tokenized the captions.\n",
      "[494000/591753] Tokenized the captions.\n",
      "[495000/591753] Tokenized the captions.\n",
      "[496000/591753] Tokenized the captions.\n",
      "[497000/591753] Tokenized the captions.\n",
      "[498000/591753] Tokenized the captions.\n",
      "[499000/591753] Tokenized the captions.\n",
      "[500000/591753] Tokenized the captions.\n",
      "[501000/591753] Tokenized the captions.\n",
      "[502000/591753] Tokenized the captions.\n",
      "[503000/591753] Tokenized the captions.\n",
      "[504000/591753] Tokenized the captions.\n",
      "[505000/591753] Tokenized the captions.\n",
      "[506000/591753] Tokenized the captions.\n",
      "[507000/591753] Tokenized the captions.\n",
      "[508000/591753] Tokenized the captions.\n",
      "[509000/591753] Tokenized the captions.\n",
      "[510000/591753] Tokenized the captions.\n",
      "[511000/591753] Tokenized the captions.\n",
      "[512000/591753] Tokenized the captions.\n",
      "[513000/591753] Tokenized the captions.\n",
      "[514000/591753] Tokenized the captions.\n",
      "[515000/591753] Tokenized the captions.\n",
      "[516000/591753] Tokenized the captions.\n",
      "[517000/591753] Tokenized the captions.\n",
      "[518000/591753] Tokenized the captions.\n",
      "[519000/591753] Tokenized the captions.\n",
      "[520000/591753] Tokenized the captions.\n",
      "[521000/591753] Tokenized the captions.\n",
      "[522000/591753] Tokenized the captions.\n",
      "[523000/591753] Tokenized the captions.\n",
      "[524000/591753] Tokenized the captions.\n",
      "[525000/591753] Tokenized the captions.\n",
      "[526000/591753] Tokenized the captions.\n",
      "[527000/591753] Tokenized the captions.\n",
      "[528000/591753] Tokenized the captions.\n",
      "[529000/591753] Tokenized the captions.\n",
      "[530000/591753] Tokenized the captions.\n",
      "[531000/591753] Tokenized the captions.\n",
      "[532000/591753] Tokenized the captions.\n",
      "[533000/591753] Tokenized the captions.\n",
      "[534000/591753] Tokenized the captions.\n",
      "[535000/591753] Tokenized the captions.\n",
      "[536000/591753] Tokenized the captions.\n",
      "[537000/591753] Tokenized the captions.\n",
      "[538000/591753] Tokenized the captions.\n",
      "[539000/591753] Tokenized the captions.\n",
      "[540000/591753] Tokenized the captions.\n",
      "[541000/591753] Tokenized the captions.\n",
      "[542000/591753] Tokenized the captions.\n",
      "[543000/591753] Tokenized the captions.\n",
      "[544000/591753] Tokenized the captions.\n",
      "[545000/591753] Tokenized the captions.\n",
      "[546000/591753] Tokenized the captions.\n",
      "[547000/591753] Tokenized the captions.\n",
      "[548000/591753] Tokenized the captions.\n",
      "[549000/591753] Tokenized the captions.\n",
      "[550000/591753] Tokenized the captions.\n",
      "[551000/591753] Tokenized the captions.\n",
      "[552000/591753] Tokenized the captions.\n",
      "[553000/591753] Tokenized the captions.\n",
      "[554000/591753] Tokenized the captions.\n",
      "[555000/591753] Tokenized the captions.\n",
      "[556000/591753] Tokenized the captions.\n",
      "[557000/591753] Tokenized the captions.\n",
      "[558000/591753] Tokenized the captions.\n",
      "[559000/591753] Tokenized the captions.\n",
      "[560000/591753] Tokenized the captions.\n",
      "[561000/591753] Tokenized the captions.\n",
      "[562000/591753] Tokenized the captions.\n",
      "[563000/591753] Tokenized the captions.\n",
      "[564000/591753] Tokenized the captions.\n",
      "[565000/591753] Tokenized the captions.\n",
      "[566000/591753] Tokenized the captions.\n",
      "[567000/591753] Tokenized the captions.\n",
      "[568000/591753] Tokenized the captions.\n",
      "[569000/591753] Tokenized the captions.\n",
      "[570000/591753] Tokenized the captions.\n",
      "[571000/591753] Tokenized the captions.\n",
      "[572000/591753] Tokenized the captions.\n",
      "[573000/591753] Tokenized the captions.\n",
      "[574000/591753] Tokenized the captions.\n",
      "[575000/591753] Tokenized the captions.\n",
      "[576000/591753] Tokenized the captions.\n",
      "[577000/591753] Tokenized the captions.\n",
      "[578000/591753] Tokenized the captions.\n",
      "[579000/591753] Tokenized the captions.\n",
      "[580000/591753] Tokenized the captions.\n",
      "[581000/591753] Tokenized the captions.\n",
      "[582000/591753] Tokenized the captions.\n",
      "[583000/591753] Tokenized the captions.\n",
      "[584000/591753] Tokenized the captions.\n",
      "[585000/591753] Tokenized the captions.\n",
      "[586000/591753] Tokenized the captions.\n",
      "[587000/591753] Tokenized the captions.\n",
      "[588000/591753] Tokenized the captions.\n",
      "[589000/591753] Tokenized the captions.\n",
      "[590000/591753] Tokenized the captions.\n",
      "[591000/591753] Tokenized the captions.\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocaburary(json='workspace/annotations/captions_train2017.json', threshold=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_image(image, shape):\n",
    "    return image.resize(shape, Image.ANTIALIAS)\n",
    "\n",
    "def reshape_images(image_path, output_path, shape):\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    \n",
    "    images = os.listdir(image_path)\n",
    "    num_im = len(images)\n",
    "    for i, im in enumerate(images):\n",
    "        with open(os.path.join(image_path, im), 'r+b') as f:\n",
    "            with Image.open(f) as image:\n",
    "                image = reshape_image(image, shape)\n",
    "                image.save(os.path.join(output_path, im), image.format)\n",
    "        \n",
    "        if (i+1) % 100 == 0 :\n",
    "            print(f\"[{i+1}/{num_im}] Resized the images and saved into : '{output_path}'\")\n",
    "\n",
    "image_path = './data/train2017/'\n",
    "output_path = './data/resized_images/'\n",
    "image_shape = [256, 256]\n",
    "reshape_images(image_path, output_path, image_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCocoDataset(Dataset):\n",
    "    def __init__(self, data_path, coco_json_path, vocabulary, transform=None):\n",
    "        self.root = data_path \n",
    "        self.coco_data = COCO(coco_json_path)\n",
    "        self.indices = list(self.coco_data.anns.keys())\n",
    "        self.vocabulary = vocabulary\n",
    "        self.transform = transform \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        coco_data = self.coco_data \n",
    "        vocabulary = self.vocabulary\n",
    "        annotation_id = self.indices[idx]\n",
    "        caption = coco_data.anns[annotation_id]['caption']\n",
    "        image_id = coco_data.anns[annotation_id]['image_id']\n",
    "        image_path = coco_data.loadings(image_id)[0]['file_name']\n",
    "\n",
    "        image = Image.open(op.path.join(self.root, image_path)).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        # Convert caption to word ids.\n",
    "        word_tokens = nltk.tokenize.word_tokenize(str(caption).lower())\n",
    "        caption = []\n",
    "        caption.append(vocabulary(''))\n",
    "        caption.extend([vocabulary(token) for token in word_tokens])\n",
    "        caption.append(vocabulary(''))\n",
    "        ground_truth = torch.Tensor(caption)\n",
    "        return image, ground_truth # x, y \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "collate_function은 DataLoader의 배치 단위에서 실행하는 함수입니다. \n",
    "텍스트의 길이가 서로 다르기 때문에 연산 효율성을 위해 collate_function을 지정하는 것입니다. (필수가 아님)\n",
    "'''\n",
    "\n",
    "def collate_function(data_batch):\n",
    "    \n",
    "    data_batch.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    imgs, caps = zip(*data_batch)\n",
    "    \n",
    "    imgs = torch.stack(imgs, 0)\n",
    "    \n",
    "    cap_lens = [len(cap) for cap in caps]\n",
    "    targets = torch.zeros(len(caps), max(cap_lens)).long()\n",
    "    for i, cap in enumerate(caps):\n",
    "        end = cap_lens[i]\n",
    "        targets[i, end] = cap[:end]\n",
    "    return imgs, targets, cap_lens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(data_path, coco_json_path, vocabulary, transform, batch_size, shuffle, num_workers):\n",
    "    coco_dataset = CustomCocoDataset(data_path=data_path, \n",
    "                                     coco_json_path=coco_json_path, \n",
    "                                     vocabulary=vocabulary, \n",
    "                                     transform=transform)\n",
    "    \n",
    "    custom_data_loader = DataLoader(dataset=coco_dataset, \n",
    "                                    batch_size=batch_size, \n",
    "                                    shuffle=shuffle, \n",
    "                                    num_workers=num_workers, \n",
    "                                    collate_fn=collate_function)\n",
    "    return custom_data_loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, embedding_size):\n",
    "        super(CNNModel, self).__init__()\n",
    "        resnet = model.resnet152(pretrained=True)\n",
    "        module_list = list(resnet.children())[:-1] # remove fully-connect layer\n",
    "        self.resnet_module = nn.Sequential(*module_list)\n",
    "        self.prediction_layer = nn.Linear(resnet.fc.in_features, embedding_size)\n",
    "        self.batch_norm = nn.BatchNorm1d(embedding_size, momentum=0.01)\n",
    "        \n",
    "    def forward(self, input_images):\n",
    "        with torch.no_grad():\n",
    "            resnet_features = self.resnet_module(input_images)\n",
    "            resnet_features = resnet_features.reshape(resnet_features.size(0), -1) # view(-1)\n",
    "            final_features = self.batch_norm(self.prediction_layer(resnet_features))\n",
    "            return final_features \n",
    "\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, vocabulary_size, num_layers, max_seq_len=20):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding_layer = nn.Embedding(vocabulary_size, embedding_size)\n",
    "        self.lstm_layer = nn.LSTM(embedding_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.prediction_layer = nn.Linear(hidden_size, vocabulary_size)\n",
    "        self.max_seq_len = max_seq_len \n",
    "    \n",
    "    def forward(self, input_features, caps, lens):\n",
    "        embeddings = self.embedding_layer(caps)\n",
    "        embeddings = torch.cat((input_features.unsqueeze(1), embeddings), 1)\n",
    "        lstm_input = pack_padded_sequence(embeddings, lens, batch_first=True)\n",
    "        hidden_variables, _ = self.lstm_layer(lstm_input)\n",
    "        model_outputs = self.prediction_layer(hidden_variables[0])\n",
    "        return model_outputs \n",
    "\n",
    "    def sample(self, input_features, lstm_states=None):\n",
    "        sampled_indices = []\n",
    "        lstm_inputs = input_features.unsqueeze(1)\n",
    "        \n",
    "        for i in range(self.max_sen_len):\n",
    "            hidden_variables, lstm_states = self.lstm_layer(lstm_inputs, lstm_states)\n",
    "            model_outputs = self.prediction_layer(hidden_variables.squeeze(1))\n",
    "            _, predicted_outputs = model_outputs.max(1)\n",
    "            sampled_indices.append(predicted_outputs)\n",
    "            lstm_inputs = self.embedding_layer(predicted_outputs)\n",
    "            lstm_inputs = lstm_inputs.unsqueeze(1)\n",
    "        sampled_indices = torch.stack(sampled_indices, 1)\n",
    "        return sampled_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "transforms = transform.Compose([ \n",
    "                                transform.RandomCrop(224),\n",
    "                                transform.RandomHorizontalFlip(), \n",
    "                                transform.ToTensor(), \n",
    "                                transform.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "\n",
    "with open('data/vocabulary.pkl', 'rb') as f:\n",
    "    vocabulary = pickle.load(f)\n",
    "\n",
    "\n",
    "custom_data_loader = get_loader(\n",
    "    'data/resized_images', \n",
    "    'annotations/captions_val2017.json', \n",
    "    vocabulary, \n",
    "    transforms,\n",
    "    128, \n",
    "    shuffle=True, \n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "encoder_model = CNNModel(256).to(device)\n",
    "decoder_model = LSTMModel(256, 512, len(vocabulary), 1).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "parameters = list(decoder_model.parameters()) + list(encoder_model.prediction_layer.parameters()) + list(encoder_model.batch_norm.parameters())\n",
    "optimizer = torch.optim.Adam(parameters, lr=1e-3)\n",
    "\n",
    "total_num_steps = len(custom_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(5):\n",
    "    for i, (imgs, caps, lens) in enumerate(custom_data_loader):\n",
    "        targets = pack_padded_sequence(caps, lens, batch_first=True)[0]\n",
    "        \n",
    "        features = encoder_model(imgs) \n",
    "        outputs = decoder_model(features, caps, lens)\n",
    "        loss = criterion(outputs, targets)\n",
    "        decoder_model.zero_grad()\n",
    "        encoder_model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(f'Epoch [{epoch}/{5}], step [{i}/{total_num_steps}], Loss: {loss.item():.4f}, Perplexity: {np.exp(loss.item()):5.4f}')\n",
    "        \n",
    "        if (i+1) % 1000 == 0:\n",
    "            torch.save(decoder_model.state_dict(), os.path.join('models_dir/', f'decoder-{epoch+1}-{i+1}.ckpt'))\n",
    "            torch.save(encoder_model.state_dict(), os.path.join('models_dir/', f'encoder-{epoch+1}-{i+1}.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file_path = 'sample.jpg'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def load_image(image_file_path, transform=None):\n",
    "    img = Image.open(image_file_path).convert('RGB')\n",
    "    img = img.resize([224, 224], Image.LANCZOS)\n",
    "    \n",
    "    if transform is not None:\n",
    "        img = transform(img).unsqueeze(0)\n",
    "    \n",
    "    return img \n",
    "\n",
    "transforms = transform.Compose([ \n",
    "                                transform.ToTensor(), \n",
    "                                transform.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/vocabularyt.pkl', 'rb') as f:\n",
    "    vocabulary = pickle.load(f)\n",
    "\n",
    "encoder_model = CNNModel(256).eval()\n",
    "decoder_model = LSTMModel(256, 512, len(vocabulary), 1)\n",
    "\n",
    "encoder_model = encoder_model.to(device)\n",
    "decoder_model = decoder_model.to(device)\n",
    "\n",
    "encoder_model.load_state_dict(torch.load('model_dir/encoder-2-3000.ckpt'))\n",
    "decoder_model.load_state_dict(torch.load('model_dir/decoder-2-3000.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = load_image(image_file_path, transform)\n",
    "img_tensor = img.to(device)\n",
    "\n",
    "features = encoder_model(img_tensor)\n",
    "sampled_indices = decoder_model.sample(features)\n",
    "\n",
    "sampled_indices = sampled_indices[0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_caption = []\n",
    "\n",
    "for token_index in sampled_indices:\n",
    "    word = vocabulary.i2w[token_index]\n",
    "    predicted_caption.append(word)\n",
    "    \n",
    "    if word == '<end>':\n",
    "        break \n",
    "    \n",
    "predicted_sentence = ' '.join(predicted_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "print(predicted_sentence)\n",
    "img = Image.open(image_file_path)\n",
    "plt.imshow(np.asarray(img))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6e5262017548a2b455e3c33d8630dbac676c51a278f558e9b73cd2cf5fc8a1af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
