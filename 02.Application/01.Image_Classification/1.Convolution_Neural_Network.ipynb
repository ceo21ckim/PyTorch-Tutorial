{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim \n",
    "from torch.utils.data import DataLoader \n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.CNN1 = nn.Conv2d(1, 16, 3, 1) # (in_channel, out_channel, kernel_size, stride)\n",
    "        self.CNN2 = nn.Conv2d(16, 32, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.1)\n",
    "        self.dropout2 = nn.Dropout2d(0.25)\n",
    "        self.fc1 = nn.Linear(4608, 64) # 12 * 12 * 32 \n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1) # criterion으로 nll_loss 사용하지 않고, BCELoss를 사용하는 경우 log_softmax를 사용하지 않습니다. \n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.CNN1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.CNN2(x)\n",
    "        x = self.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        outputs = self.log_softmax(x)\n",
    "        return outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, data_loader, optim, criterion, epoch):\n",
    "    model.train()\n",
    "    for i, (x, y) in enumerate(data_loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optim.zero_grad()\n",
    "        pred_y = model(x)\n",
    "        loss = criterion(pred_y, y) \n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        if i % 10 == 0 :\n",
    "            print(f'epoch: {epoch}\\t {i}/{len(data_loader)} training loss: {loss.item():.6f}')\n",
    "\n",
    "def test(model, device, dataloader, criterion):\n",
    "    model.eval()\n",
    "    loss, correct = 0, 0 \n",
    "    with torch.no_grad():\n",
    "        for (x, y) in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            pred_y = model(x)\n",
    "            \n",
    "            loss += criterion(pred_y, y)\n",
    "            \n",
    "            pred = pred_y.argmax(dim=1, keepdim = True)\n",
    "            \n",
    "            correct += pred.eq(y.view_as(pred)).sum().item() # view_as 함수는 form을 맞춰줄때 사용합니다. \n",
    "    \n",
    "    loss /= len(dataloader.dataset)\n",
    "    print(f'\\nTest dataset: Overall Loss {loss:.4f}, Overall Accuracy: {100. * correct / len(dataloader.dataset):.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset =  datasets.MNIST('../data', train=True, download=True, \n",
    "                            transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                                            transforms.Normalize((0.1302, ), (0.3069, ))]))\n",
    "\n",
    "testset =  datasets.MNIST('../data', train=False,\n",
    "                            transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                                            transforms.Normalize((0.1302, ), (0.3069, ))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(testset, batch_size=500, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "device = torch.device( 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = ConvNet()\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=0.5)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\t 0/1875 training loss: 2.301563\n",
      "epoch: 1\t 10/1875 training loss: 1.999758\n",
      "epoch: 1\t 20/1875 training loss: 1.404254\n",
      "epoch: 1\t 30/1875 training loss: 1.206638\n",
      "epoch: 1\t 40/1875 training loss: 0.798878\n",
      "epoch: 1\t 50/1875 training loss: 0.854382\n",
      "epoch: 1\t 60/1875 training loss: 0.916494\n",
      "epoch: 1\t 70/1875 training loss: 0.722387\n",
      "epoch: 1\t 80/1875 training loss: 1.117116\n",
      "epoch: 1\t 90/1875 training loss: 0.958758\n",
      "epoch: 1\t 100/1875 training loss: 0.912771\n",
      "epoch: 1\t 110/1875 training loss: 0.748333\n",
      "epoch: 1\t 120/1875 training loss: 0.910245\n",
      "epoch: 1\t 130/1875 training loss: 0.555541\n",
      "epoch: 1\t 140/1875 training loss: 0.889906\n",
      "epoch: 1\t 150/1875 training loss: 0.994307\n",
      "epoch: 1\t 160/1875 training loss: 0.876377\n",
      "epoch: 1\t 170/1875 training loss: 0.860758\n",
      "epoch: 1\t 180/1875 training loss: 0.770217\n",
      "epoch: 1\t 190/1875 training loss: 0.518235\n",
      "epoch: 1\t 200/1875 training loss: 0.948469\n",
      "epoch: 1\t 210/1875 training loss: 0.471645\n",
      "epoch: 1\t 220/1875 training loss: 0.887531\n",
      "epoch: 1\t 230/1875 training loss: 0.584910\n",
      "epoch: 1\t 240/1875 training loss: 0.395916\n",
      "epoch: 1\t 250/1875 training loss: 0.581313\n",
      "epoch: 1\t 260/1875 training loss: 1.017546\n",
      "epoch: 1\t 270/1875 training loss: 0.785210\n",
      "epoch: 1\t 280/1875 training loss: 0.500267\n",
      "epoch: 1\t 290/1875 training loss: 0.578893\n",
      "epoch: 1\t 300/1875 training loss: 0.807376\n",
      "epoch: 1\t 310/1875 training loss: 0.700378\n",
      "epoch: 1\t 320/1875 training loss: 0.461268\n",
      "epoch: 1\t 330/1875 training loss: 0.919174\n",
      "epoch: 1\t 340/1875 training loss: 0.732788\n",
      "epoch: 1\t 350/1875 training loss: 0.556639\n",
      "epoch: 1\t 360/1875 training loss: 0.647165\n",
      "epoch: 1\t 370/1875 training loss: 0.793160\n",
      "epoch: 1\t 380/1875 training loss: 0.352504\n",
      "epoch: 1\t 390/1875 training loss: 0.885367\n",
      "epoch: 1\t 400/1875 training loss: 0.747218\n",
      "epoch: 1\t 410/1875 training loss: 0.599031\n",
      "epoch: 1\t 420/1875 training loss: 0.427131\n",
      "epoch: 1\t 430/1875 training loss: 0.610540\n",
      "epoch: 1\t 440/1875 training loss: 0.772805\n",
      "epoch: 1\t 450/1875 training loss: 0.748374\n",
      "epoch: 1\t 460/1875 training loss: 0.468919\n",
      "epoch: 1\t 470/1875 training loss: 0.877201\n",
      "epoch: 1\t 480/1875 training loss: 0.395421\n",
      "epoch: 1\t 490/1875 training loss: 0.389389\n",
      "epoch: 1\t 500/1875 training loss: 0.284128\n",
      "epoch: 1\t 510/1875 training loss: 0.715359\n",
      "epoch: 1\t 520/1875 training loss: 0.814693\n",
      "epoch: 1\t 530/1875 training loss: 0.747096\n",
      "epoch: 1\t 540/1875 training loss: 1.004497\n",
      "epoch: 1\t 550/1875 training loss: 0.612039\n",
      "epoch: 1\t 560/1875 training loss: 0.736612\n",
      "epoch: 1\t 570/1875 training loss: 0.300376\n",
      "epoch: 1\t 580/1875 training loss: 0.782503\n",
      "epoch: 1\t 590/1875 training loss: 0.859290\n",
      "epoch: 1\t 600/1875 training loss: 0.189708\n",
      "epoch: 1\t 610/1875 training loss: 0.504388\n",
      "epoch: 1\t 620/1875 training loss: 0.675514\n",
      "epoch: 1\t 630/1875 training loss: 0.944488\n",
      "epoch: 1\t 640/1875 training loss: 0.565539\n",
      "epoch: 1\t 650/1875 training loss: 0.512492\n",
      "epoch: 1\t 660/1875 training loss: 0.971615\n",
      "epoch: 1\t 670/1875 training loss: 0.524503\n",
      "epoch: 1\t 680/1875 training loss: 0.789017\n",
      "epoch: 1\t 690/1875 training loss: 0.442441\n",
      "epoch: 1\t 700/1875 training loss: 0.779994\n",
      "epoch: 1\t 710/1875 training loss: 0.683922\n",
      "epoch: 1\t 720/1875 training loss: 0.730312\n",
      "epoch: 1\t 730/1875 training loss: 0.845372\n",
      "epoch: 1\t 740/1875 training loss: 0.811909\n",
      "epoch: 1\t 750/1875 training loss: 0.703028\n",
      "epoch: 1\t 760/1875 training loss: 0.864246\n",
      "epoch: 1\t 770/1875 training loss: 0.494020\n",
      "epoch: 1\t 780/1875 training loss: 0.442066\n",
      "epoch: 1\t 790/1875 training loss: 0.652752\n",
      "epoch: 1\t 800/1875 training loss: 0.600431\n",
      "epoch: 1\t 810/1875 training loss: 1.224806\n",
      "epoch: 1\t 820/1875 training loss: 0.628691\n",
      "epoch: 1\t 830/1875 training loss: 0.807062\n",
      "epoch: 1\t 840/1875 training loss: 0.649987\n",
      "epoch: 1\t 850/1875 training loss: 0.464642\n",
      "epoch: 1\t 860/1875 training loss: 0.779446\n",
      "epoch: 1\t 870/1875 training loss: 0.701253\n",
      "epoch: 1\t 880/1875 training loss: 0.720600\n",
      "epoch: 1\t 890/1875 training loss: 0.617254\n",
      "epoch: 1\t 900/1875 training loss: 0.465052\n",
      "epoch: 1\t 910/1875 training loss: 0.947567\n",
      "epoch: 1\t 920/1875 training loss: 0.810609\n",
      "epoch: 1\t 930/1875 training loss: 0.413627\n",
      "epoch: 1\t 940/1875 training loss: 1.213014\n",
      "epoch: 1\t 950/1875 training loss: 0.399181\n",
      "epoch: 1\t 960/1875 training loss: 0.670569\n",
      "epoch: 1\t 970/1875 training loss: 0.741235\n",
      "epoch: 1\t 980/1875 training loss: 0.814603\n",
      "epoch: 1\t 990/1875 training loss: 0.728488\n",
      "epoch: 1\t 1000/1875 training loss: 0.437884\n",
      "epoch: 1\t 1010/1875 training loss: 0.486529\n",
      "epoch: 1\t 1020/1875 training loss: 0.460714\n",
      "epoch: 1\t 1030/1875 training loss: 0.431352\n",
      "epoch: 1\t 1040/1875 training loss: 0.449179\n",
      "epoch: 1\t 1050/1875 training loss: 0.627430\n",
      "epoch: 1\t 1060/1875 training loss: 0.661803\n",
      "epoch: 1\t 1070/1875 training loss: 0.592570\n",
      "epoch: 1\t 1080/1875 training loss: 0.641230\n",
      "epoch: 1\t 1090/1875 training loss: 0.715349\n",
      "epoch: 1\t 1100/1875 training loss: 0.766065\n",
      "epoch: 1\t 1110/1875 training loss: 0.773168\n",
      "epoch: 1\t 1120/1875 training loss: 0.357903\n",
      "epoch: 1\t 1130/1875 training loss: 0.500300\n",
      "epoch: 1\t 1140/1875 training loss: 0.407378\n",
      "epoch: 1\t 1150/1875 training loss: 0.509786\n",
      "epoch: 1\t 1160/1875 training loss: 0.600868\n",
      "epoch: 1\t 1170/1875 training loss: 0.862791\n",
      "epoch: 1\t 1180/1875 training loss: 0.868979\n",
      "epoch: 1\t 1190/1875 training loss: 0.742084\n",
      "epoch: 1\t 1200/1875 training loss: 0.448171\n",
      "epoch: 1\t 1210/1875 training loss: 1.038578\n",
      "epoch: 1\t 1220/1875 training loss: 0.316475\n",
      "epoch: 1\t 1230/1875 training loss: 0.710901\n",
      "epoch: 1\t 1240/1875 training loss: 0.795962\n",
      "epoch: 1\t 1250/1875 training loss: 0.703811\n",
      "epoch: 1\t 1260/1875 training loss: 0.473318\n",
      "epoch: 1\t 1270/1875 training loss: 0.694506\n",
      "epoch: 1\t 1280/1875 training loss: 0.885828\n",
      "epoch: 1\t 1290/1875 training loss: 0.763893\n",
      "epoch: 1\t 1300/1875 training loss: 1.118454\n",
      "epoch: 1\t 1310/1875 training loss: 0.437684\n",
      "epoch: 1\t 1320/1875 training loss: 0.583637\n",
      "epoch: 1\t 1330/1875 training loss: 0.593737\n",
      "epoch: 1\t 1340/1875 training loss: 0.376254\n",
      "epoch: 1\t 1350/1875 training loss: 0.749470\n",
      "epoch: 1\t 1360/1875 training loss: 0.929749\n",
      "epoch: 1\t 1370/1875 training loss: 0.723390\n",
      "epoch: 1\t 1380/1875 training loss: 0.495064\n",
      "epoch: 1\t 1390/1875 training loss: 0.672910\n",
      "epoch: 1\t 1400/1875 training loss: 0.775478\n",
      "epoch: 1\t 1410/1875 training loss: 0.934218\n",
      "epoch: 1\t 1420/1875 training loss: 0.627712\n",
      "epoch: 1\t 1430/1875 training loss: 0.432932\n",
      "epoch: 1\t 1440/1875 training loss: 0.693066\n",
      "epoch: 1\t 1450/1875 training loss: 0.517671\n",
      "epoch: 1\t 1460/1875 training loss: 0.455731\n",
      "epoch: 1\t 1470/1875 training loss: 0.592339\n",
      "epoch: 1\t 1480/1875 training loss: 0.661260\n",
      "epoch: 1\t 1490/1875 training loss: 1.162956\n",
      "epoch: 1\t 1500/1875 training loss: 0.578081\n",
      "epoch: 1\t 1510/1875 training loss: 0.645357\n",
      "epoch: 1\t 1520/1875 training loss: 0.941864\n",
      "epoch: 1\t 1530/1875 training loss: 0.368221\n",
      "epoch: 1\t 1540/1875 training loss: 1.100463\n",
      "epoch: 1\t 1550/1875 training loss: 0.801295\n",
      "epoch: 1\t 1560/1875 training loss: 0.452492\n",
      "epoch: 1\t 1570/1875 training loss: 0.875373\n",
      "epoch: 1\t 1580/1875 training loss: 0.502994\n",
      "epoch: 1\t 1590/1875 training loss: 0.939623\n",
      "epoch: 1\t 1600/1875 training loss: 0.919087\n",
      "epoch: 1\t 1610/1875 training loss: 0.577317\n",
      "epoch: 1\t 1620/1875 training loss: 0.651482\n",
      "epoch: 1\t 1630/1875 training loss: 0.894566\n",
      "epoch: 1\t 1640/1875 training loss: 0.652657\n",
      "epoch: 1\t 1650/1875 training loss: 0.868909\n",
      "epoch: 1\t 1660/1875 training loss: 0.438111\n",
      "epoch: 1\t 1670/1875 training loss: 0.611141\n",
      "epoch: 1\t 1680/1875 training loss: 0.797447\n",
      "epoch: 1\t 1690/1875 training loss: 0.444098\n",
      "epoch: 1\t 1700/1875 training loss: 0.869040\n",
      "epoch: 1\t 1710/1875 training loss: 0.795703\n",
      "epoch: 1\t 1720/1875 training loss: 0.221173\n",
      "epoch: 1\t 1730/1875 training loss: 0.443165\n",
      "epoch: 1\t 1740/1875 training loss: 0.663721\n",
      "epoch: 1\t 1750/1875 training loss: 0.643889\n",
      "epoch: 1\t 1760/1875 training loss: 0.768459\n",
      "epoch: 1\t 1770/1875 training loss: 0.360286\n",
      "epoch: 1\t 1780/1875 training loss: 0.594051\n",
      "epoch: 1\t 1790/1875 training loss: 0.626629\n",
      "epoch: 1\t 1800/1875 training loss: 0.575345\n",
      "epoch: 1\t 1810/1875 training loss: 0.601023\n",
      "epoch: 1\t 1820/1875 training loss: 0.455351\n",
      "epoch: 1\t 1830/1875 training loss: 0.580187\n",
      "epoch: 1\t 1840/1875 training loss: 0.502319\n",
      "epoch: 1\t 1850/1875 training loss: 0.419033\n",
      "epoch: 1\t 1860/1875 training loss: 0.627408\n",
      "epoch: 1\t 1870/1875 training loss: 0.743538\n",
      "\n",
      "Test dataset: Overall Loss 0.0001, Overall Accuracy: 98.34\n",
      "epoch: 2\t 0/1875 training loss: 0.827496\n",
      "epoch: 2\t 10/1875 training loss: 0.620087\n",
      "epoch: 2\t 20/1875 training loss: 0.529449\n",
      "epoch: 2\t 30/1875 training loss: 0.657446\n",
      "epoch: 2\t 40/1875 training loss: 0.512049\n",
      "epoch: 2\t 50/1875 training loss: 0.486720\n",
      "epoch: 2\t 60/1875 training loss: 0.492118\n",
      "epoch: 2\t 70/1875 training loss: 0.866186\n",
      "epoch: 2\t 80/1875 training loss: 0.640297\n",
      "epoch: 2\t 90/1875 training loss: 0.522401\n",
      "epoch: 2\t 100/1875 training loss: 0.776757\n",
      "epoch: 2\t 110/1875 training loss: 0.723742\n",
      "epoch: 2\t 120/1875 training loss: 0.790629\n",
      "epoch: 2\t 130/1875 training loss: 0.368255\n",
      "epoch: 2\t 140/1875 training loss: 0.802329\n",
      "epoch: 2\t 150/1875 training loss: 0.591293\n",
      "epoch: 2\t 160/1875 training loss: 0.913760\n",
      "epoch: 2\t 170/1875 training loss: 0.365260\n",
      "epoch: 2\t 180/1875 training loss: 0.680150\n",
      "epoch: 2\t 190/1875 training loss: 0.362506\n",
      "epoch: 2\t 200/1875 training loss: 0.499658\n",
      "epoch: 2\t 210/1875 training loss: 0.520741\n",
      "epoch: 2\t 220/1875 training loss: 0.637969\n",
      "epoch: 2\t 230/1875 training loss: 0.579795\n",
      "epoch: 2\t 240/1875 training loss: 0.648157\n",
      "epoch: 2\t 250/1875 training loss: 0.447655\n",
      "epoch: 2\t 260/1875 training loss: 0.580254\n",
      "epoch: 2\t 270/1875 training loss: 0.581598\n",
      "epoch: 2\t 280/1875 training loss: 0.506342\n",
      "epoch: 2\t 290/1875 training loss: 0.460143\n",
      "epoch: 2\t 300/1875 training loss: 0.505898\n",
      "epoch: 2\t 310/1875 training loss: 0.652614\n",
      "epoch: 2\t 320/1875 training loss: 0.581203\n",
      "epoch: 2\t 330/1875 training loss: 0.742468\n",
      "epoch: 2\t 340/1875 training loss: 0.512789\n",
      "epoch: 2\t 350/1875 training loss: 0.582031\n",
      "epoch: 2\t 360/1875 training loss: 0.796063\n",
      "epoch: 2\t 370/1875 training loss: 0.326019\n",
      "epoch: 2\t 380/1875 training loss: 0.647956\n",
      "epoch: 2\t 390/1875 training loss: 0.723387\n",
      "epoch: 2\t 400/1875 training loss: 0.520001\n",
      "epoch: 2\t 410/1875 training loss: 0.446258\n",
      "epoch: 2\t 420/1875 training loss: 0.508462\n",
      "epoch: 2\t 430/1875 training loss: 0.298087\n",
      "epoch: 2\t 440/1875 training loss: 0.573502\n",
      "epoch: 2\t 450/1875 training loss: 0.761826\n",
      "epoch: 2\t 460/1875 training loss: 0.646174\n",
      "epoch: 2\t 470/1875 training loss: 0.587154\n",
      "epoch: 2\t 480/1875 training loss: 0.568877\n",
      "epoch: 2\t 490/1875 training loss: 0.382587\n",
      "epoch: 2\t 500/1875 training loss: 0.513610\n",
      "epoch: 2\t 510/1875 training loss: 0.436097\n",
      "epoch: 2\t 520/1875 training loss: 0.480916\n",
      "epoch: 2\t 530/1875 training loss: 0.645627\n",
      "epoch: 2\t 540/1875 training loss: 1.080565\n",
      "epoch: 2\t 550/1875 training loss: 0.500552\n",
      "epoch: 2\t 560/1875 training loss: 0.779746\n",
      "epoch: 2\t 570/1875 training loss: 0.365244\n",
      "epoch: 2\t 580/1875 training loss: 0.737123\n",
      "epoch: 2\t 590/1875 training loss: 0.232085\n",
      "epoch: 2\t 600/1875 training loss: 0.432264\n",
      "epoch: 2\t 610/1875 training loss: 0.707194\n",
      "epoch: 2\t 620/1875 training loss: 0.291873\n",
      "epoch: 2\t 630/1875 training loss: 0.507951\n",
      "epoch: 2\t 640/1875 training loss: 0.577085\n",
      "epoch: 2\t 650/1875 training loss: 0.434495\n",
      "epoch: 2\t 660/1875 training loss: 0.303174\n",
      "epoch: 2\t 670/1875 training loss: 0.790748\n",
      "epoch: 2\t 680/1875 training loss: 0.646412\n",
      "epoch: 2\t 690/1875 training loss: 0.365357\n",
      "epoch: 2\t 700/1875 training loss: 0.649440\n",
      "epoch: 2\t 710/1875 training loss: 0.870911\n",
      "epoch: 2\t 720/1875 training loss: 0.466992\n",
      "epoch: 2\t 730/1875 training loss: 0.362761\n",
      "epoch: 2\t 740/1875 training loss: 0.514417\n",
      "epoch: 2\t 750/1875 training loss: 0.507370\n",
      "epoch: 2\t 760/1875 training loss: 0.767946\n",
      "epoch: 2\t 770/1875 training loss: 0.514806\n",
      "epoch: 2\t 780/1875 training loss: 0.727179\n",
      "epoch: 2\t 790/1875 training loss: 0.374000\n",
      "epoch: 2\t 800/1875 training loss: 0.576903\n",
      "epoch: 2\t 810/1875 training loss: 0.491082\n",
      "epoch: 2\t 820/1875 training loss: 0.666218\n",
      "epoch: 2\t 830/1875 training loss: 0.641489\n",
      "epoch: 2\t 840/1875 training loss: 0.434163\n",
      "epoch: 2\t 850/1875 training loss: 0.941287\n",
      "epoch: 2\t 860/1875 training loss: 0.862992\n",
      "epoch: 2\t 870/1875 training loss: 0.374805\n",
      "epoch: 2\t 880/1875 training loss: 0.942134\n",
      "epoch: 2\t 890/1875 training loss: 0.871113\n",
      "epoch: 2\t 900/1875 training loss: 0.721583\n",
      "epoch: 2\t 910/1875 training loss: 0.469391\n",
      "epoch: 2\t 920/1875 training loss: 0.583428\n",
      "epoch: 2\t 930/1875 training loss: 0.757788\n",
      "epoch: 2\t 940/1875 training loss: 0.693057\n",
      "epoch: 2\t 950/1875 training loss: 0.858996\n",
      "epoch: 2\t 960/1875 training loss: 0.623749\n",
      "epoch: 2\t 970/1875 training loss: 0.290667\n",
      "epoch: 2\t 980/1875 training loss: 0.815897\n",
      "epoch: 2\t 990/1875 training loss: 0.530159\n",
      "epoch: 2\t 1000/1875 training loss: 0.607076\n",
      "epoch: 2\t 1010/1875 training loss: 0.504181\n",
      "epoch: 2\t 1020/1875 training loss: 0.454840\n",
      "epoch: 2\t 1030/1875 training loss: 0.377094\n",
      "epoch: 2\t 1040/1875 training loss: 0.472742\n",
      "epoch: 2\t 1050/1875 training loss: 0.382069\n",
      "epoch: 2\t 1060/1875 training loss: 0.392309\n",
      "epoch: 2\t 1070/1875 training loss: 0.944019\n",
      "epoch: 2\t 1080/1875 training loss: 0.907925\n",
      "epoch: 2\t 1090/1875 training loss: 0.439883\n",
      "epoch: 2\t 1100/1875 training loss: 1.016154\n",
      "epoch: 2\t 1110/1875 training loss: 1.002627\n",
      "epoch: 2\t 1120/1875 training loss: 0.652816\n",
      "epoch: 2\t 1130/1875 training loss: 1.092865\n",
      "epoch: 2\t 1140/1875 training loss: 0.431740\n",
      "epoch: 2\t 1150/1875 training loss: 0.572063\n",
      "epoch: 2\t 1160/1875 training loss: 0.578147\n",
      "epoch: 2\t 1170/1875 training loss: 0.653993\n",
      "epoch: 2\t 1180/1875 training loss: 0.875558\n",
      "epoch: 2\t 1190/1875 training loss: 0.758646\n",
      "epoch: 2\t 1200/1875 training loss: 0.791353\n",
      "epoch: 2\t 1210/1875 training loss: 0.742466\n",
      "epoch: 2\t 1220/1875 training loss: 0.587651\n",
      "epoch: 2\t 1230/1875 training loss: 0.653041\n",
      "epoch: 2\t 1240/1875 training loss: 0.658539\n",
      "epoch: 2\t 1250/1875 training loss: 0.725127\n",
      "epoch: 2\t 1260/1875 training loss: 0.665565\n",
      "epoch: 2\t 1270/1875 training loss: 0.585903\n",
      "epoch: 2\t 1280/1875 training loss: 0.425917\n",
      "epoch: 2\t 1290/1875 training loss: 0.726127\n",
      "epoch: 2\t 1300/1875 training loss: 0.432058\n",
      "epoch: 2\t 1310/1875 training loss: 0.715862\n",
      "epoch: 2\t 1320/1875 training loss: 0.427494\n",
      "epoch: 2\t 1330/1875 training loss: 0.686972\n",
      "epoch: 2\t 1340/1875 training loss: 0.552805\n",
      "epoch: 2\t 1350/1875 training loss: 0.429499\n",
      "epoch: 2\t 1360/1875 training loss: 0.598605\n",
      "epoch: 2\t 1370/1875 training loss: 0.815062\n",
      "epoch: 2\t 1380/1875 training loss: 0.767299\n",
      "epoch: 2\t 1390/1875 training loss: 0.653450\n",
      "epoch: 2\t 1400/1875 training loss: 0.512430\n",
      "epoch: 2\t 1410/1875 training loss: 0.584909\n",
      "epoch: 2\t 1420/1875 training loss: 0.694538\n",
      "epoch: 2\t 1430/1875 training loss: 0.224397\n",
      "epoch: 2\t 1440/1875 training loss: 0.739922\n",
      "epoch: 2\t 1450/1875 training loss: 0.452356\n",
      "epoch: 2\t 1460/1875 training loss: 0.363302\n",
      "epoch: 2\t 1470/1875 training loss: 0.696551\n",
      "epoch: 2\t 1480/1875 training loss: 0.726378\n",
      "epoch: 2\t 1490/1875 training loss: 0.537571\n",
      "epoch: 2\t 1500/1875 training loss: 0.440498\n",
      "epoch: 2\t 1510/1875 training loss: 0.686298\n",
      "epoch: 2\t 1520/1875 training loss: 0.648812\n",
      "epoch: 2\t 1530/1875 training loss: 0.874793\n",
      "epoch: 2\t 1540/1875 training loss: 0.457175\n",
      "epoch: 2\t 1550/1875 training loss: 0.738306\n",
      "epoch: 2\t 1560/1875 training loss: 0.436602\n",
      "epoch: 2\t 1570/1875 training loss: 0.810774\n",
      "epoch: 2\t 1580/1875 training loss: 0.756081\n",
      "epoch: 2\t 1590/1875 training loss: 0.643019\n",
      "epoch: 2\t 1600/1875 training loss: 0.771425\n",
      "epoch: 2\t 1610/1875 training loss: 0.609733\n",
      "epoch: 2\t 1620/1875 training loss: 0.669816\n",
      "epoch: 2\t 1630/1875 training loss: 0.575849\n",
      "epoch: 2\t 1640/1875 training loss: 0.648158\n",
      "epoch: 2\t 1650/1875 training loss: 0.498869\n",
      "epoch: 2\t 1660/1875 training loss: 0.667137\n",
      "epoch: 2\t 1670/1875 training loss: 0.449164\n",
      "epoch: 2\t 1680/1875 training loss: 0.509186\n",
      "epoch: 2\t 1690/1875 training loss: 0.764107\n",
      "epoch: 2\t 1700/1875 training loss: 0.371706\n",
      "epoch: 2\t 1710/1875 training loss: 0.410496\n",
      "epoch: 2\t 1720/1875 training loss: 0.882918\n",
      "epoch: 2\t 1730/1875 training loss: 0.657597\n",
      "epoch: 2\t 1740/1875 training loss: 0.561883\n",
      "epoch: 2\t 1750/1875 training loss: 0.652963\n",
      "epoch: 2\t 1760/1875 training loss: 0.358705\n",
      "epoch: 2\t 1770/1875 training loss: 0.511802\n",
      "epoch: 2\t 1780/1875 training loss: 0.507497\n",
      "epoch: 2\t 1790/1875 training loss: 0.806168\n",
      "epoch: 2\t 1800/1875 training loss: 0.977626\n",
      "epoch: 2\t 1810/1875 training loss: 0.654395\n",
      "epoch: 2\t 1820/1875 training loss: 0.656991\n",
      "epoch: 2\t 1830/1875 training loss: 0.507185\n",
      "epoch: 2\t 1840/1875 training loss: 0.739434\n",
      "epoch: 2\t 1850/1875 training loss: 0.800248\n",
      "epoch: 2\t 1860/1875 training loss: 0.717436\n",
      "epoch: 2\t 1870/1875 training loss: 0.736623\n",
      "\n",
      "Test dataset: Overall Loss 0.0001, Overall Accuracy: 98.63\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 3):\n",
    "    train(model, device, train_dataloader, optimizer, criterion, epoch)\n",
    "    test(model, device, test_dataloader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(sample_data, sample_target) = next(iter(test_dataloader))\n",
    "\n",
    "plt.imshow(sample_data[0][0], cmap='gray', interpolation='none');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Prediction is : 5\n",
      "Ground Truth is : 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:1320: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "print(f'Model Prediction is : {model(sample_data).data.max(1)[1][0]}')\n",
    "print(f'Ground Truth is : {sample_target[0]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6e5262017548a2b455e3c33d8630dbac676c51a278f558e9b73cd2cf5fc8a1af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
