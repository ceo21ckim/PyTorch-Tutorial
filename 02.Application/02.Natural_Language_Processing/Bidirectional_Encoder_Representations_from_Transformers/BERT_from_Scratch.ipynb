{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, os\n",
    "import tqdm \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import random \n",
    "\n",
    "import torch\n",
    "from torch import nn, optim \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join('data', 'yelp_sample.csv')\n",
    "dataset = pd.read_csv(data_path, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "train, valid = train_test_split(dataset, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_score(x):\n",
    "    if x >= 3.5 : return 1\n",
    "    elif x < 3.5 : return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.user_id = train.loc[:, 'user_id'].astype('category')\n",
    "train.business_id = train.loc[:, 'business_id'].astype('category')\n",
    "\n",
    "total_user_category = train.loc[:, 'user_id'].cat.categories\n",
    "total_rest_category = train.loc[:, 'business_id'].cat.categories\n",
    "\n",
    "valid.user_id = valid.loc[:, 'user_id'].astype('category')\n",
    "valid.business_id = valid.loc[:, 'business_id'].astype('category')\n",
    "\n",
    "test.user_id = test.loc[:, 'user_id'].astype('category')\n",
    "test.business_id = test.loc[:, 'business_id'].astype('category')\n",
    "\n",
    "valid.user_id= valid.loc[:, 'user_id'].cat.set_categories(total_user_category)\n",
    "valid.business_id = valid.loc[:, 'business_id'].cat.set_categories(total_rest_category)\n",
    "\n",
    "test.user_id = test.loc[:, 'user_id'].cat.set_categories(total_user_category)\n",
    "test.business_id = test.loc[:, 'business_id'].cat.set_categories(total_rest_category)\n",
    "\n",
    "train.user_id = train.loc[:, 'user_id'].cat.codes\n",
    "train.business_id = train.loc[:, 'business_id'].cat.codes\n",
    "\n",
    "valid.user_id = valid.loc[:, 'user_id'].cat.codes\n",
    "valid.business_id = valid.loc[:, 'business_id'].cat.codes\n",
    "\n",
    "test.user_id = test.loc[:, 'user_id'].cat.codes\n",
    "test.business_id = test.loc[:, 'business_id'].cat.codes\n",
    "\n",
    "train.loc[:,'stars'] = train.loc[:, 'stars'].apply(sentiment_score)\n",
    "valid.loc[:, 'stars'] = valid.loc[:, 'stars'].apply(sentiment_score)\n",
    "test.loc[:, 'stars'] = test.loc[:, 'stars'].apply(sentiment_score)\n",
    "\n",
    "train = train.dropna().reset_index(drop=True)\n",
    "valid = valid.dropna().reset_index(drop=True)\n",
    "test = test.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[:, 'text'] = train.loc[:, 'text'].apply(lambda x: '[CLS] ' + str(x) + ' [SEP]' )\n",
    "valid.loc[:, 'text'] = valid.loc[:, 'text'].apply(lambda x: '[CLS] ' + str(x) + ' [SEP]' )\n",
    "test.loc[:, 'text'] = test.loc[:, 'text'].apply(lambda x: '[CLS] ' + str(x) + ' [SEP]' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at huawei-noah/TinyBERT_General_4L_312D were not used when initializing BertModel: ['fit_denses.1.weight', 'fit_denses.4.weight', 'cls.seq_relationship.weight', 'fit_denses.2.bias', 'cls.predictions.transform.LayerNorm.bias', 'fit_denses.3.bias', 'fit_denses.0.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'fit_denses.4.bias', 'cls.predictions.transform.dense.weight', 'fit_denses.3.weight', 'cls.predictions.bias', 'fit_denses.2.weight', 'cls.predictions.transform.LayerNorm.weight', 'fit_denses.0.weight', 'cls.predictions.transform.dense.bias', 'fit_denses.1.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_model = BertModel.from_pretrained('huawei-noah/TinyBERT_General_4L_312D')\n",
    "tokenizer = BertTokenizer.from_pretrained('huawei-noah/TinyBERT_General_4L_312D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, bertmodel):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bertmodel\n",
    "        self.classifier = nn.Linear(312, 1) # tinybert=312, bert-base=768\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        output = self.bert(input_ids, attention_mask = attention_mask, token_type_ids = token_type_ids)\n",
    "        pooler = output['pooler_output']\n",
    "        pooler = self.dropout(pooler)\n",
    "        fc_layer = self.classifier(pooler)\n",
    "        return fc_layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERTClassifier(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 312, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 312)\n",
       "      (token_type_embeddings): Embedding(2, 312)\n",
       "      (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (key): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (value): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=312, out_features=1200, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1200, out_features=312, bias=True)\n",
       "            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (key): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (value): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=312, out_features=1200, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1200, out_features=312, bias=True)\n",
       "            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (key): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (value): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=312, out_features=1200, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1200, out_features=312, bias=True)\n",
       "            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (key): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (value): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=312, out_features=1200, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1200, out_features=312, bias=True)\n",
       "            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=312, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = BERTClassifier(bert_model)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer):\n",
    "        self.tokenizer = tokenizer \n",
    "        self.data = dataframe \n",
    "        self.reviews = dataframe.text \n",
    "        self.labels = dataframe.stars\n",
    "        self.max_seq_length = 512\n",
    "\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        review = self.reviews[idx]\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            review, \n",
    "            None,\n",
    "            add_special_tokens=True, \n",
    "            max_length=self.max_seq_length, \n",
    "            padding='max_length', \n",
    "            return_token_type_ids=True, \n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        input_ids = inputs['input_ids']\n",
    "        masks = inputs['attention_mask']\n",
    "        token_type_ids = inputs['token_type_ids']\n",
    "\n",
    "        return (\n",
    "            torch.tensor(input_ids, dtype=torch.long), # token_ids\n",
    "            torch.tensor(masks, dtype=torch.long), # attention_mask\n",
    "            torch.tensor(token_type_ids, dtype=torch.long), # token_type_ids\n",
    "            torch.tensor(self.labels[idx], dtype = float) # labels\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BERTDataset( train, tokenizer)\n",
    "valid_dataset = BERTDataset( valid, tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, num_workers=1)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=32, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time \n",
    "    elapsed_mins = int(elapsed_time/60)\n",
    "    elapsed_secs = elapsed_time - elapsed_mins*60 \n",
    "\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "def calc_accuracy(pred_y, true_y):\n",
    "    return ((pred_y > 0.5) == true_y).sum().detach().cpu().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "learning_rate = 1e-5\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer =optim.Adam(model.parameters(), lr = learning_rate, weight_decay = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...:   0%|          | 0/282 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_loss_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "best_loss = float('inf')\n",
    "set_seed(42)\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    train_loss = 0 \n",
    "    train_acc = 0\n",
    "\n",
    "    test_loss = 0\n",
    "    test_acc = 0 \n",
    "\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    for batch in tqdm.tqdm(train_dataloader, desc = 'training...'):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        inputs = {'input_ids':      batch[0], \n",
    "                  'attention_mask': batch[1], \n",
    "                  'token_type_ids':     batch[2]}\n",
    "        label = batch[3]\n",
    "\n",
    "        pred_y = model(**inputs).squeeze()\n",
    "        loss = criterion(pred_y, label)\n",
    "\n",
    "        train_acc += calc_accuracy(pred_y, label)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_acc /= len(train_dataset)\n",
    "    train_loss /= len(train_dataset)\n",
    "    train_acc_list.append(train_acc)\n",
    "    train_loss_list.append(train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm.tqdm(valid_dataloader, desc = 'evaluating...'):\n",
    "            batch = tuple(t.to(args.device) for t in batch)\n",
    "\n",
    "            inputs = {'input_ids':      batch[0], \n",
    "                      'attention_mask': batch[1], \n",
    "                      'token_type_ids':     batch[2]}\n",
    "            label = batch[3]\n",
    "\n",
    "            pred_y = model(**inputs).squeeze()\n",
    "            loss = criterion(pred_y, label)\n",
    "\n",
    "            test_acc += calc_accuracy(pred_y, label)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    test_acc /= len(valid_dataset)\n",
    "    test_loss /= len(valid_dataset)\n",
    "    test_acc_list.append(test_acc)\n",
    "    test_loss_list.append(test_loss)\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_mins, elapsed_secs = epoch_time(start_time, end_time)\n",
    "    print(f'epoch [{epoch}/{num_epochs}], elapsed time: {elapsed_mins}m, {elapsed_secs:.2f}s')\n",
    "    print(f'train loss: {train_loss:.4f}\\ttrain accuracy: {train_acc*100:.2f}%')\n",
    "    print(f'test loss: {test_loss:.4f}\\ttest accuracy: {test_acc*100:.2f}%\\n')\n",
    "\n",
    "    if best_loss > test_loss :\n",
    "        best_loss = test_loss \n",
    "\n",
    "        torch.save(model.state_dict(), 'model_parameters.pt')\n",
    "    \n",
    "df = pd.DataFrame([train_loss_list, test_loss_list, train_acc_list, test_acc_list], index = ['train_loss', 'test_loss', 'train_acc', 'test_acc']).T \n",
    "\n",
    "df.to_csv(f'results.csv', encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "87f8120500551a546151f2b0ca958f68b0ffffb73102cc5528ecad6003f8fe87"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
